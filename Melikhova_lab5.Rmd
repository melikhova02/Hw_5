---
title: "R Notebook"
output:
  html_document:
    df_print: paged
  html_notebook: default
  pdf_document: default
  word_document: default
---

## Математическое моделирование


### Кросс-валидация и бутстреп      

В данной задаче выполняются следующие пункты: 

* оценка точности модели методом перекрёстной выборки;    
* методом проверочной выборки;    
* методом перекрёстной проверки по отдельным наблюдениям (LOOCV);   
* методом k-кратной перекрёстной проверки;   
* применение бутстреп для оценки точности статистического параметра и оценок параметров модели   


*Модели*: линейная регрессия, kNN.   
*Данные*: статистика стоимости жилья в пригороде Бостона.   


```{r}
library('GGally') # графики совместного разброса переменных
library('lmtest') # тесты остатков регрессионных моделей
library('FNN') # алгоритм kNN
library('mlbench')
library('MASS')
library('ISLR')
library('boot')
my.seed <- 1
```


## Метод перекрёстной проверки

```{r}
# Пример на данных по районам: Boston 
data(Boston) # открываем данные
?Boston
Boston <- Boston[,-c(2,5,6,8,9,10,11,12,13,14)]
Boston$chas <- as.factor(Boston$chas)

# графики разброса
ggpairs(Boston)
```

## Метод проверочной выборки 

Он состоит в том, что мы отбираем одну тестовую выборку и будем считать на ней ошибку модели.    

```{r}
n <- nrow(Boston)

# доля обучающей выборки
train.percent <- 0.5


# выбрать наблюдения в обучающую выборку
set.seed(my.seed)
inTrain <- sample(n, n*train.percent) 
inTrain
```

Построим модели для проверки точности. Вид моделей:   
$$
\hat{crim} = f(age,indus, chas)
$$
**Линейная модель**: $\hat{crim} = \hat{\beta}_0 + \hat{\beta}_1 \cdot age + \hat{\beta}_2 \cdot indus + \hat{\beta}_3 \cdot chas$.

``` {r}
# присоединить таблицу с данными: названия стоблцов будут доступны напрямую
attach(Boston)
# подгонка линейной модели на обучающей выборке

fit.lm.1 <- lm(crim ~ age + indus + chas, subset = inTrain)

# считаем MSE на тестовой выборке
mean((crim[-inTrain] - predict(fit.lm.1, Boston[-inTrain,]))^2)
# отсоединить таблицу с данными
detach(Boston)
```


Строим первую **квадратичную модель**: $\hat{crim} = \hat{\beta}_0 + \hat{\beta}_1 \cdot age^2 + \hat{\beta}_2 \cdot indus+ \hat{\beta}_3 \cdot chas$.   

```{r}
# присоединить таблицу с данными: названия стоблцов будут доступны напрямую
attach(Boston)
# подгонка линейной модели на обучающей выборке
fit.lm.2 <- lm(crim ~ poly(age,2)+ indus + chas ,subset = inTrain)

# считаем MSE на тестовой выборке
mean((crim[-inTrain] - predict(fit.lm.2, Boston[-inTrain,]))^2)

# отсоединить таблицу с данными
detach(Boston)
```

Строим вторую **квадратичную модель**: $\hat{crim} = \hat{\beta}_0 + \hat{\beta}_1 \cdot indus^2 + \hat{\beta}_2 \cdot age+ \hat{\beta}_3 \cdot chas$.   

```{r}
# присоединить таблицу с данными: названия стоблцов будут доступны напрямую
attach(Boston)
# подгонка линейной модели на обучающей выборке
fit.lm.3 <- lm(crim ~ age + poly(indus,2) + chas ,subset = inTrain)

# считаем MSE на тестовой выборке
mean((crim[-inTrain] - predict(fit.lm.3, Boston[-inTrain,]))^2)

# отсоединить таблицу с данными
detach(Boston)
```

Строим первую **кубическую модель**: $\hat{crim} = \hat{\beta}_0 + \hat{\beta}_1 \cdot age^3 + \hat{\beta}_2 \cdot indus + \hat{\beta}_3 \cdot chas$.   

```{r}
# присоединить таблицу с данными: названия стоблцов будут доступны напрямую
attach(Boston)
# подгонка линейной модели на обучающей выборке
fit.lm.5 <- lm(crim ~ poly(age,3)+ indus + chas ,subset = inTrain)

# считаем MSE на тестовой выборке
mean((crim[-inTrain] - predict(fit.lm.5, Boston[-inTrain,]))^2)

# отсоединить таблицу с данными
detach(Boston)
```

Строим вторую **кубическую модель**: $\hat{crim} = \hat{\beta}_0 + \hat{\beta}_1 \cdot indus^3 + \hat{\beta}_2 \cdot age + \hat{\beta}_3 \cdot chas$.   

```{r}
# присоединить таблицу с данными: названия стоблцов будут доступны напрямую
attach(Boston)
# подгонка линейной модели на обучающей выборке
fit.lm.4 <- lm(crim ~ age + poly(indus,3) + chas ,subset = inTrain)

# считаем MSE на тестовой выборке
mean((crim[-inTrain] - predict(fit.lm.4, Boston[-inTrain,]))^2)

# отсоединить таблицу с данными
detach(Boston)
```

Ошибка последней кубической модели оказалась наименьшей из всех построенных, следовательно она и будет являться наиболее пригодной для прогнозирования. 

### Перекрёстная проверка по отдельным наблюдениям (LOOCV)

Это самый затратный в вычислительном плане метод, но и самый надёжный в плане оценки ошибки вне выборки. Попробуем применить его к линейной модели.    

```{r}
# подгонка линейной модели на обучающей выборке
fit.glm <- glm(crim ~ age + indus + chas, data = Boston)
# считаем LOOCV-ошибку
cv.err <- cv.glm(Boston, fit.glm)
# результат: первое число -- по формуле LOOCV-ошибки,
#  второе -- с поправкой на смещение
cv.err$delta[1]
```  

Теперь оценим точность полиномиальных моделей, меняя степень, в которой стоит регрессор.   
Вид модели 1: crim ~ poly(age,i) + indus + chas

```{r}
# вектор с LOOCV-ошибками
cv.err.loocv <- rep(0, 5)
names(cv.err.loocv) <- 1:5
# цикл по степеням полиномов
for (i in 1:5){
  fit.glm <- glm(crim ~ poly(age,i) + indus + chas, data = Boston)
  cv.err.loocv[i] <- cv.glm(Boston, fit.glm)$delta[1]
}
# результат
cv.err.loocv
```

Вид модели 2: crim ~ age + poly(indus,i) + chas

```{r}
# вектор с LOOCV-ошибками
cv.err.loocv <- rep(0, 5)
names(cv.err.loocv) <- 1:5
# цикл по степеням полиномов
for (i in 1:5){
  fit.glm <- glm(crim ~ age + poly(indus,i) + chas, data = Boston)
  cv.err.loocv[i] <- cv.glm(Boston, fit.glm)$delta[1]
}
# результат
cv.err.loocv
```

### k-кратная перекрёстная проверка

K-кратная кросс-валидация -- компромисс между методом проверочной выборки и LOOCV. Оценка ошибки вне выборки ближе к правде, по сравнению с проверочной выборкой, а объём вычислений меньше, чем при LOOCV. Проведём 5-кратную и 10-кратную кросс-валидацию моделей разных степеней вида модели 2 ("crim ~ age + poly(indus,i) + chas"), так как предыдущий пункт показал, что ошибки в данном случае оказались ниже.     

Проведём 5-кратную кросс-валидацию:  

```{r}
# оценим точность полиномиальных моделей, меняя степень
# вектор с ошибками по 5-кратной кросс-валидации
cv.err.k.fold <- rep(0, 5)
names(cv.err.k.fold) <- 1:5
# цикл по степеням полиномов
for (i in 1:5){
  fit.glm <- glm(crim ~ age + poly(indus,i) + chas, data = Boston)
  cv.err.k.fold[i] <- cv.glm(Boston, fit.glm,
                             K = 5)$delta[1]
}
# результат
cv.err.k.fold

```

Проведём 10-кратную кросс-валидацию:  

```{r}
# оценим точность полиномиальных моделей, меняя степень
# вектор с ошибками по 10-кратной кросс-валидации
cv.err.k.fold <- rep(0, 10)
names(cv.err.k.fold) <- 1:10
# цикл по степеням полиномов
for (i in 1:10){
  fit.glm <- glm(crim ~ age + poly(indus,i) + chas, data = Boston)
  cv.err.k.fold[i] <- cv.glm(Boston, fit.glm,
                             K = 10)$delta[1]
}
# результат
cv.err.k.fold
```

Для сравнения напомним результаты расчёта MSE методом проверочной выборки:   

```{r}
err.test
```


Опираясь на результаты расчётов с кросс-валидацией, можно заключить, что на самом деле ошибка вне выборки у линейной модели выше, чем показывала MSE на тестовой выборке. Методы кросс-валидации сходятся на одной и той же модели: на первом этапе (первая степень) модель показывает наименьшую ошибку.


## Бутстреп   


### Точность оценки параметра регрессии   

При построении модели регрессии проблемы в остатках приводят к неверной оценке ошибок параметров. Обойти эту проблему можно, применив для расчёта этих ошибок бутстреп.   

```{r}
# Оценивание точности линейной регрессионной модели ----------------------------
# оценить стандартные ошибки параметров модели 
#  mpg = beta_0 + beta_1 * horsepower с помощью бутстрепа,
#  сравнить с оценками ошибок по МНК
# функция для расчёта коэффициентов ПЛР по выборке из данных
boot.fn <- function(data, index){
  coef(lm(crim ~ age + indus + chas, data = data, subset = index))
}
boot.fn(Boston, 1:n)
# пример применения функции к бутстреп-выборке
set.seed(my.seed)
boot.fn(Boston, sample(n, n, replace = T))
# применяем функцию boot для вычисления стандартных ошибок параметров
#  (1000 выборок с повторами)
boot(Boston, boot.fn, 1000)
# сравним с МНК
attach(Boston)
summary(lm(crim ~ age + indus + chas))$coef
detach(Boston)
#оценим наилучшую найденную модель
boot.fn.2 <- function(data, index){
  coef(lm(crim ~ age + poly(indus,3) + chas, data = data, subset = index))
}
# применим функцию к 1000 бутсреп-выборкам
set.seed(my.seed)
boot(Boston, boot.fn, 1000)
```

В модели регрессии, для которой проводился расчёт, похоже, не нарушаются требования к остаткам, и оценки стандартных ошибок параметров, рассчитанные по МНК, очень близки к ошибкам этих же параметров, полученных бутстрепом.   
